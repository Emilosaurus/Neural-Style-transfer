{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11944258,"sourceType":"datasetVersion","datasetId":7508789},{"sourceId":11944310,"sourceType":"datasetVersion","datasetId":7508833}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Neural Style Transfer in PyTorch\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom torchvision import transforms, models\nimport matplotlib.pyplot as plt\nimport copy\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Image loading and preprocessing\nloader = transforms.Compose([\n    transforms.Resize((300, 300)),\n    transforms.ToTensor()\n])\n\nunloader = transforms.ToPILImage()  # To convert tensor to PIL Image for display\n\ndef load_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone().squeeze(0)\n    image = unloader(image)\n    plt.imshow(image)\n    if title:\n        plt.title(title)\n    plt.pause(0.001)\n\n# Load content and style images\ncontent_img = load_image(\"/kaggle/input/input-image/BMW lights 3.jpg\")\nstyle_img = load_image(\"/kaggle/input/style-image/images.jpg\")\n\nassert content_img.size() == style_img.size(), \"Style and content images must be the same size\"\n\n# Define content and style loss modules\nclass ContentLoss(nn.Module):\n    def __init__(self, target):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n\n    def forward(self, input):\n        self.loss = nn.functional.mse_loss(input, self.target)\n        return input\n\nclass StyleLoss(nn.Module):\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = self.gram_matrix(target_feature).detach()\n\n    def gram_matrix(self, input):\n        a, b, c, d = input.size()\n        features = input.view(a * b, c * d)\n        G = torch.mm(features, features.t())\n        return G.div(a * b * c * d)\n\n    def forward(self, input):\n        G = self.gram_matrix(input)\n        self.loss = nn.functional.mse_loss(G, self.target)\n        return input\n\n# Load pretrained VGG19\ncnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n\n# VGG normalization\ncnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        self.mean = mean.view(-1, 1, 1)\n        self.std = std.view(-1, 1, 1)\n\n    def forward(self, img):\n        return (img - self.mean) / self.std\n\n# Layers to use\ncontent_layers = ['conv_4']\nstyle_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                                style_img, content_img,\n                                content_layers, style_layers):\n    cnn = copy.deepcopy(cnn)\n\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n\n    content_losses = []\n    style_losses = []\n\n    model = nn.Sequential(normalization)\n\n    i = 0\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n    model = model[:i+1]\n\n    return model, style_losses, content_losses\n\n# Run style transfer\ndef run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=300,\n                       style_weight=1000000, content_weight=1):\n    model, style_losses, content_losses = get_style_model_and_losses(\n        cnn, normalization_mean, normalization_std, style_img, content_img,\n        content_layers, style_layers)\n\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            input_img.data.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            loss = style_score * style_weight + content_score * content_weight\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run[0]))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return loss\n\n        optimizer.step(closure)\n\n    input_img.data.clamp_(0, 1)\n\n    return input_img\n\n# Start from content image\ninput_img = content_img.clone()\noutput = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)\n\n# Display output\nplt.figure()\nimshow(output, title='Output Image')\nplt.ioff()\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:28:14.262654Z","iopub.execute_input":"2025-05-25T10:28:14.263276Z","iopub.status.idle":"2025-05-25T10:28:26.874195Z","shell.execute_reply.started":"2025-05-25T10:28:14.263240Z","shell.execute_reply":"2025-05-25T10:28:26.873506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Neural Style Transfer on Multiple Images using PyTorch\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Image loader and preprocessor\ndef load_image(image_path, max_size=512):\n    image = Image.open(image_path).convert('RGB')\n    size = max(image.size)\n    if size > max_size:\n        size = max_size\n    in_transform = transforms.Compose([\n        transforms.Resize((size, size)),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x[:3, :, :]),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                             (0.229, 0.224, 0.225))])\n    image = in_transform(image).unsqueeze(0)\n    return image.to(device)\n\n# Convert tensor to image\ndef im_convert(tensor):\n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1, 2, 0)\n    image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n    image = image.clip(0, 1)\n    return image\n\n# Content and style loss\nclass ContentLoss(nn.Module):\n    def __init__(self, target):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n\n    def forward(self, x):\n        self.loss = nn.functional.mse_loss(x, self.target)\n        return x\n\nclass StyleLoss(nn.Module):\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = self.gram_matrix(target_feature).detach()\n\n    def gram_matrix(self, input):\n        b, c, h, w = input.size()\n        features = input.view(b * c, h * w)\n        G = torch.mm(features, features.t())\n        return G.div(b * c * h * w)\n\n    def forward(self, x):\n        G = self.gram_matrix(x)\n        self.loss = nn.functional.mse_loss(G, self.target)\n        return x\n\n# Build style transfer model\ndef get_style_model_and_losses(cnn, style_img, content_img):\n    cnn = cnn.to(device).eval()\n    content_layers = ['conv_4']\n    style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\n    content_losses = []\n    style_losses = []\n\n    model = nn.Sequential().to(device)\n    i = 0\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = f\"conv_{i}\"\n        elif isinstance(layer, nn.ReLU):\n            name = f\"relu_{i}\"\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = f\"pool_{i}\"\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = f\"bn_{i}\"\n        else:\n            continue\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(f\"content_loss_{i}\", content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            target = model(style_img).detach()\n            style_loss = StyleLoss(target)\n            model.add_module(f\"style_loss_{i}\", style_loss)\n            style_losses.append(style_loss)\n\n    return model, style_losses, content_losses\n\n# Style transfer function\ndef run_style_transfer(cnn, style_img, content_img, input_img, num_steps=300, style_weight=1000000, content_weight=1):\n    model, style_losses, content_losses = get_style_model_and_losses(cnn, style_img, content_img)\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n\n    run = [0]\n    while run[0] <= num_steps:\n        def closure():\n            input_img.data.clamp_(0, 1)\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = sum(sl.loss for sl in style_losses)\n            content_score = sum(cl.loss for cl in content_losses)\n            loss = style_score * style_weight + content_score * content_weight\n            loss.backward()\n            run[0] += 1\n            return style_score + content_score\n        optimizer.step(closure)\n    input_img.data.clamp_(0, 1)\n    return input_img\n\n# === Execution Start ===\n\n# Load style image\nstyle_image_path = \"/kaggle/input/style-image/images.jpg\"\nstyle_img = load_image(style_image_path)\n\n# Directory of content images\ncontent_dir = \"/kaggle/input/input-image\"\noutput_dir = \"styled_outputs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Load model\ncnn = models.vgg19(pretrained=True).features\n\n# Style transfer for all content images\nfor image_name in os.listdir(content_dir):\n    if image_name.endswith(('.jpg', '.png', '.jpeg')):\n        content_img = load_image(os.path.join(content_dir, image_name))\n        input_img = content_img.clone()\n        output = run_style_transfer(cnn, style_img, content_img, input_img)\n        result = im_convert(output)\n        plt.imsave(os.path.join(output_dir, f\"styled_{image_name}\"), result)\n\nprint(\"âœ… Style transfer completed for all images.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:17:19.374938Z","iopub.execute_input":"2025-05-25T10:17:19.375365Z","iopub.status.idle":"2025-05-25T10:17:19.397524Z","shell.execute_reply.started":"2025-05-25T10:17:19.375338Z","shell.execute_reply":"2025-05-25T10:17:19.396635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:19:35.949126Z","iopub.execute_input":"2025-05-25T10:19:35.949716Z","iopub.status.idle":"2025-05-25T10:19:36.564058Z","shell.execute_reply.started":"2025-05-25T10:19:35.949691Z","shell.execute_reply":"2025-05-25T10:19:36.563148Z"}},"outputs":[],"execution_count":null}]}